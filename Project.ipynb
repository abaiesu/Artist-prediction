{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2e7c7a2a",
   "metadata": {},
   "source": [
    "Data gathering"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dd14f535",
   "metadata": {},
   "source": [
    "analysis of 2 or 3 different kind of algorithms we will use: kNN, maxent, logistic regression, deep learning, and support vector machines."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3aa4615c",
   "metadata": {},
   "source": [
    "each method has a different pre processing and strengths and interpretatio\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d63b68e8",
   "metadata": {},
   "source": [
    "kNN- tokenizing, all lower case, removing common words, lematisize, tagging words as preposition/conjunction/etc,  TF IDF"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "53fc48e5",
   "metadata": {},
   "source": [
    "Do we remove stop words, or not?\n",
    "Do we stem or lemmatize our text data, or leave the words as is?\n",
    "Is basic tokenization enough, or do we need to support special edge cases through the use of regex?\n",
    "Do we stick with English words only or do we allow for other languages?\n",
    "Do we use the entire vocabulary, or just limit the model to a subset of the most frequently used words? If so, how many?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f04e1ad-8ec8-4dcf-a1b6-09dcfc18d0cb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting Levenshtein\n",
      "  Downloading Levenshtein-0.21.0-cp38-cp38-win_amd64.whl (101 kB)\n",
      "Collecting rapidfuzz<4.0.0,>=2.3.0\n",
      "  Downloading rapidfuzz-3.0.0-cp38-cp38-win_amd64.whl (1.8 MB)\n",
      "Installing collected packages: rapidfuzz, Levenshtein\n",
      "Successfully installed Levenshtein-0.21.0 rapidfuzz-3.0.0\n",
      "Requirement already satisfied: pandas in c:\\users\\aarry\\anaconda3\\lib\\site-packages (1.2.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\aarry\\anaconda3\\lib\\site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\aarry\\anaconda3\\lib\\site-packages (from pandas) (2021.1)\n",
      "Requirement already satisfied: numpy>=1.16.5 in c:\\users\\aarry\\anaconda3\\lib\\site-packages (from pandas) (1.20.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\aarry\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
      "Requirement already satisfied: nltk in c:\\users\\aarry\\anaconda3\\lib\\site-packages (3.6.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\aarry\\anaconda3\\lib\\site-packages (from nltk) (1.0.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\aarry\\anaconda3\\lib\\site-packages (from nltk) (4.59.0)\n",
      "Requirement already satisfied: click in c:\\users\\aarry\\anaconda3\\lib\\site-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied: regex in c:\\users\\aarry\\anaconda3\\lib\\site-packages (from nltk) (2021.4.4)\n",
      "Requirement already satisfied: Levenshtein in c:\\users\\aarry\\anaconda3\\lib\\site-packages (0.21.0)\n",
      "Requirement already satisfied: rapidfuzz<4.0.0,>=2.3.0 in c:\\users\\aarry\\anaconda3\\lib\\site-packages (from Levenshtein) (3.0.0)\n",
      "Collecting textblob\n",
      "  Using cached textblob-0.17.1-py2.py3-none-any.whl (636 kB)\n",
      "Requirement already satisfied: nltk>=3.1 in c:\\users\\aarry\\anaconda3\\lib\\site-packages (from textblob) (3.6.1)\n",
      "Requirement already satisfied: regex in c:\\users\\aarry\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (2021.4.4)\n",
      "Requirement already satisfied: click in c:\\users\\aarry\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (7.1.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\aarry\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (4.59.0)\n",
      "Requirement already satisfied: joblib in c:\\users\\aarry\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (1.0.1)\n",
      "Installing collected packages: textblob\n",
      "Successfully installed textblob-0.17.1\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\aarry\\anaconda3\\lib\\site-packages (0.24.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\aarry\\anaconda3\\lib\\site-packages (from scikit-learn) (2.1.0)\n",
      "Requirement already satisfied: scipy>=0.19.1 in c:\\users\\aarry\\anaconda3\\lib\\site-packages (from scikit-learn) (1.6.2)\n",
      "Requirement already satisfied: numpy>=1.13.3 in c:\\users\\aarry\\anaconda3\\lib\\site-packages (from scikit-learn) (1.20.1)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\aarry\\anaconda3\\lib\\site-packages (from scikit-learn) (1.0.1)\n",
      "Collecting xgboost\n",
      "  Using cached xgboost-1.7.5-py3-none-win_amd64.whl (70.9 MB)\n",
      "Requirement already satisfied: numpy in c:\\users\\aarry\\anaconda3\\lib\\site-packages (from xgboost) (1.20.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\aarry\\anaconda3\\lib\\site-packages (from xgboost) (1.6.2)\n",
      "Installing collected packages: xgboost\n",
      "Successfully installed xgboost-1.7.5\n"
     ]
    }
   ],
   "source": [
    "#!pip install Levenshtein\n",
    "#!pip install pandas\n",
    "#!pip install nltk\n",
    "#!pip install Levenshtein\n",
    "#!pip install textblob\n",
    "#!pip install scikit-learn\n",
    "#!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b901159",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#imports\n",
    "\n",
    "import numpy as np\n",
    "from textblob import TextBlob\n",
    "import random\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import neighbors\n",
    "import sklearn.neighbors\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import re\n",
    "import os\n",
    "import nltk\n",
    "import Levenshtein\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from scipy.interpolate import make_interp_spline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from collections import Counter\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.ensemble import BaggingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9afd29c-c478-4555-b2b4-18c0d8565a91",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Aarry\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Aarry\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\Aarry\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\Aarry\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0feeb9a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "def zip_folder(folder_path, zip_path):\n",
    "    shutil.make_archive(zip_path, 'zip', folder_path)\n",
    "\n",
    "# Example usage\n",
    "#folder_path = '/home/jovyan/persistent/CSE204/FINAL_PROJECT/Raw_Songs_DataBase'  # Replace with the path to your folder\n",
    "#zip_path = '/home/jovyan/persistent/CSE204/FINAL_PROJECT'  # Replace with the desired path and name for the zip file\n",
    "\n",
    "#zip_folder(folder_path, zip_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2617e95c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_file(file_path):\n",
    "    \n",
    "    with open(file_path, 'r') as file:\n",
    "        text = file.read()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a55dfde-41ad-4612-8f0a-d2fecfaf1b99",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#DEFINE A MAX NUMBER OF ARTISTS\n",
    "NUMBER_OF_ARTISTS = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27ea3c6d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-8237da439d8b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     29\u001b[0m                 \u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_to_artist_songs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msong\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m                 \u001b[0mtitles\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msong\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m                 \u001b[0mlyrics\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mread_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m                 \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlyrics\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m                 \u001b[0msongs_per_artist\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mart\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-a2d1c1ed546c>\u001b[0m in \u001b[0;36mread_file\u001b[1;34m(file_path)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mread_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m         \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\_bootlocale.py\u001b[0m in \u001b[0;36mgetpreferredencoding\u001b[1;34m(do_setlocale)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplatform\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"win\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[1;32mdef\u001b[0m \u001b[0mgetpreferredencoding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdo_setlocale\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutf8_mode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;34m'UTF-8'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#get the data\n",
    "\n",
    "path_to_songs = os.path.join(os.getcwd(), \"Raw_Songs_DataBase2\")\n",
    "\n",
    "artists = os.listdir(path_to_songs)\n",
    "artists_all = [art for art in artists if art[0] != \".\"]\n",
    "#artists = random.sample(artists_all, NUMBER_OF_ARTISTS)\n",
    "artists = sorted(artists)\n",
    "\n",
    "x = []\n",
    "RID_features = []\n",
    "y = []\n",
    "\n",
    "songs_per_artist={art : 0 for art in artists}\n",
    "\n",
    "titles = []\n",
    "\n",
    "for art in artists: \n",
    "    \n",
    "\n",
    "    path_to_artist = os.path.join(path_to_songs, art)\n",
    "    path_to_artist_songs = os.path.join(path_to_artist, \"Songs\")\n",
    "    #path_to_rid = os.path.join(path_to_artist, \"Songs\")\n",
    "    \n",
    "    songs = os.listdir(path_to_artist_songs)\n",
    "    for song in songs:\n",
    "        if song[0] != \".\":\n",
    "            if song[-3:] != \"RID\":\n",
    "                path = os.path.join(path_to_artist_songs, song)\n",
    "                titles.append(song)\n",
    "                lyrics =  read_file(path)\n",
    "                x.append(lyrics)\n",
    "                songs_per_artist[art] += 1\n",
    "                y.append(art)\n",
    "            else: #is a RID file\n",
    "                path = os.path.join(path_to_artist_songs, song)\n",
    "                text =  read_file(path)\n",
    "                RID_features.append(text)\n",
    "\n",
    "    \n",
    "arts_dict = {}\n",
    "for i in range(len(artists)):\n",
    "    arts_dict[artists[i]] = i\n",
    "    \n",
    "arts_dict_op = {i: artist for artist, i in arts_dict.items()}\n",
    "    \n",
    "for i in range(len(y)):\n",
    "    art = y[i]\n",
    "    y[i] = int(arts_dict[art])\n",
    "\n",
    "x = np.array(x)\n",
    "y = np.array(y) \n",
    "titles = np.array(titles)\n",
    "\n",
    "assert len(x) == len(RID_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92d7f0d-2b8c-4e5b-aa65-b8195442cedf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "save_dir = os.path.join(os.getcwd(), \"saved_plots\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16089d15",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#remove all that is into paranthesis in the titles\n",
    "for i in range(len(titles)):\n",
    "    titles[i] = re.sub(r'\\([^()]+\\)', '', titles[i]).strip()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "72308d4b",
   "metadata": {},
   "source": [
    "Now let us get some data about the songs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35cc63d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f'Number of songs : {len(x)}')\n",
    "print(f'Number of artists : {len(artists)}')\n",
    "print(f'Average number of songs per artist : {int(len(x)/len(artists))}')\n",
    "\n",
    "max_value = max(songs_per_artist.values())\n",
    "min_value = min(songs_per_artist.values())\n",
    "\n",
    "print(f\"Maximum number of songs: {max_value}\")\n",
    "print(f\"Minimum number of songs: {min_value}\")\n",
    "\n",
    "# Define the bins for different song count ranges\n",
    "max_bin = ((max_value // 20) + 1) * 20\n",
    "bins = [i*20 for i in range((max_bin//20) + 1)]\n",
    "\n",
    "# Initialize counts for each bin\n",
    "bin_counts = [0] * (len(bins) - 1)\n",
    "\n",
    "# Count the number of artists falling into each bin\n",
    "for value in songs_per_artist.values():\n",
    "    for i in range(len(bins) - 1):\n",
    "        if bins[i] <= value < bins[i + 1]:\n",
    "            bin_counts[i] += 1\n",
    "            break\n",
    "\n",
    "fig = plt.figure(figsize=(12, 6))\n",
    "fig.subplots_adjust(left=0.1, right=0.9, top=0.9, bottom=0.1)\n",
    "\n",
    "bar_lefts = range(len(bin_counts))\n",
    "bar_heights = bin_counts\n",
    "\n",
    "plt.bar(bar_lefts, bar_heights, width=1.0, facecolor='gray')\n",
    "\n",
    "plt.xlabel('Song Count Ranges', fontsize=20, labelpad=15)\n",
    "plt.ylabel('Number of Artists', fontsize=20, labelpad=15)\n",
    "\n",
    "plt.xticks(range(len(bin_counts)), [f'{bins[i]}-{bins[i+1]}' for i in range(len(bins)-1)], fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "\n",
    "plt.xlim(-0.5, len(bin_counts) - 0.5)\n",
    "\n",
    "save_path = os.path.join(save_dir, f'distribution_songs_start.png')\n",
    "plt.savefig(save_path, bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7782366",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def flatten(text):\n",
    "    text = [word for sentence in text for word in sentence]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f254d8df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def pre_processing(text):\n",
    "    \n",
    "    text = text.lower() #lowercase\n",
    "    \n",
    "    #split in lines\n",
    "    text = text.split(\"\\n\") \n",
    "    \n",
    "    #remove useless punctuation\n",
    "    punctuation = r'[/()-.?!,\";{}]'\n",
    "    text= [re.sub(punctuation,\"\", sentence) for sentence in text]\n",
    "    \n",
    "    #seperate each word\n",
    "    text = [line.split(' ') for line in text]\n",
    "    \n",
    "    #flatten the list of lists\n",
    "\n",
    "    \n",
    "    #remove empty lines\n",
    "    text = [line for line in text if len(line)!=0]\n",
    "    \n",
    "    \n",
    "    #remove all expressions in between brackets and empty words\n",
    "    text = [ [word.strip() for word in line if len(word) != 0 and word[0]!= \"[\"] for line in text]\n",
    "    \n",
    "    #remobe empty lines again\n",
    "    text = [line for line in text if len(line)!=0]\n",
    "    nb_lines = len(text)\n",
    "    \n",
    "    #stemmatization\n",
    "    ps = PorterStemmer()\n",
    "    text = [[ps.stem(word) for word in line] for line in text]\n",
    "    \n",
    "    nb_words = 0\n",
    "    for line in text:\n",
    "        nb_words += len(line)\n",
    "\n",
    "\n",
    "    return text, nb_lines, nb_words\n",
    "\n",
    "def title_pre_processing(title):\n",
    "    title = title.lower()  # lowercase\n",
    "    #title = re.sub(r'\\.txt$', '', title)  # remove .txt extension\n",
    "    punctuation = r'[-.?!,\";/]'\n",
    "    title = re.sub(punctuation, \"\", title)\n",
    "    title = re.sub(r'[\\(\\[].*?[\\)\\]]', '', title)\n",
    "    title = title.strip()\n",
    "    return title"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6a8832dc",
   "metadata": {},
   "source": [
    "Let's vizualize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f3fc80",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = []\n",
    "for i in range(len(x)):\n",
    "    song = x[i]\n",
    "    point = list(pre_processing(song))\n",
    "    point.append(song)\n",
    "    t = title_pre_processing(titles[i])\n",
    "    point.append(t)\n",
    "    point.append(y[i])\n",
    "    data.append(point)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e50564",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data, columns=['Lyrics', 'Nb_Lines', 'Nb_Words', 'Raw_Lyrics', 'Title', 'Artist'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cfb7a5c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_box_graph(features, save=None):\n",
    "    \n",
    "\n",
    "    \n",
    "    nb_graphs = len(features)\n",
    "    if nb_graphs == 1:\n",
    "        feature = features[0]\n",
    "        to_plot = []\n",
    "        for art in sample:\n",
    "            d1 = df.loc[df['Artist'] == art, feature]\n",
    "            to_plot.append(d1.tolist())\n",
    "        plt.boxplot(to_plot)\n",
    "        xtick_labels = [arts_dict_op[i] for i in sample]\n",
    "        plt.gca().set_xticklabels(xtick_labels, rotation='vertical', fontsize=8)\n",
    "        plt.title(feature)\n",
    "        if save != None:\n",
    "            save_path = os.path.join(save_dir, f'plot_{features}.png')\n",
    "            plt.savefig(save_path, bbox_inches='tight')\n",
    "\n",
    "        plt.show()\n",
    "    else:\n",
    "        fig, graphs = plt.subplots(1, nb_graphs, figsize=(10, 4), gridspec_kw={'wspace': 0.3})\n",
    "    \n",
    "        plots = [[] for _ in range(nb_graphs)]\n",
    "\n",
    "        for art in sample:\n",
    "            for i in range(len(features)):\n",
    "                feature = features[i]\n",
    "                d = df.loc[df['Artist'] == art, feature]\n",
    "                plots[i].append(d)\n",
    "        \n",
    "        for i in range(len(plots)):\n",
    "            graphs[i].boxplot(plots[i])\n",
    "\n",
    "        xtick_labels = [arts_dict_op[i] for i in sample]\n",
    "        \n",
    "        for i in range(len(features)):\n",
    "            feature = features[i]\n",
    "            graphs[i].set_xticklabels(xtick_labels, rotation='vertical', fontsize=8)\n",
    "            graphs[i].set_title(feature)\n",
    "\n",
    "        if save != None:\n",
    "            save_path = os.path.join(save_dir, f'plot_{features}.png')\n",
    "            plt.savefig(save_path, bbox_inches='tight')\n",
    "\n",
    "        \n",
    "        plt.show()\n",
    "        \n",
    "def plot_bar_graph(feature, save = None):\n",
    "\n",
    "    to_plot = []\n",
    "    for art in sample:\n",
    "        d1 = df.loc[df['Artist'] == art, feature]\n",
    "        to_plot.append(d1.mean())\n",
    "\n",
    "   \n",
    "    plt.bar(np.arange(len(to_plot)), to_plot, color='gray')\n",
    "    xtick_labels = [arts_dict_op[i] for i in sample]\n",
    "    plt.xticks(np.arange(len(to_plot)), xtick_labels, rotation='vertical', fontsize=8)\n",
    "    plt.title(feature)\n",
    "    \n",
    "    if save != None:\n",
    "        save_path = os.path.join(save_dir, f'plot_{feature}.png')\n",
    "        plt.savefig(save_path)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed95c57",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#let's add the RID columns\n",
    "def RID_dict(text):\n",
    "    text = text.split(\"\\n\")\n",
    "    text = [re.findall(r'\\d+(?:\\.\\d+)?|\\D+', t) for t in text]\n",
    "    #text = [t.split(' ') for t in text if len(t)!=0 and t[0].isupper()]\n",
    "    text = [[word.strip() for word in t if re.match(r'^[A-Z0-9.]', word)] for t in text]\n",
    "    text = [arr for arr in text if len(arr) != 0]\n",
    "    summary = text[-4:-1]\n",
    "    details = text[:-4]\n",
    "    lenght = text[-1][0]\n",
    "    details = [ [word.replace(' ', ':') for word in line] for line in details]\n",
    "    summary = [ [s.replace(':', '').strip() for s in line] for line in summary]\n",
    "    summary = [ [s.replace(':', '').strip() for s in line] for line in summary]\n",
    "\n",
    "    details = {d[0] : (int(d[1]))/(int(lenght)) for d in details}\n",
    "    #details = {d[0] : int(d[1]) for d in details}\n",
    "    summary = {d[0] : float(d[1]) for d in summary}\n",
    "    \n",
    "    return details, summary\n",
    "\n",
    "RID_DETAILS = {}\n",
    "RID_SUMMARY = {'RID_PRIMARY': [], 'RID_SECONDARY' : [], 'RID_EMOTIONS' : []}\n",
    "\n",
    "#first let's get the labels\n",
    "for rid in RID_features:\n",
    "    d, s = RID_dict(rid)\n",
    "    for key in d.keys():\n",
    "        if key not in RID_DETAILS.keys():\n",
    "            RID_DETAILS['RID_'+key] = []\n",
    "\n",
    "#now fill in the values\n",
    "for rid in RID_features:\n",
    "    d, s = RID_dict(rid)\n",
    "    for key in RID_DETAILS.keys():\n",
    "        if key[4:] in d.keys():\n",
    "            RID_DETAILS[key].append(d[key[4:]])\n",
    "        else:\n",
    "            RID_DETAILS[key].append(0)\n",
    "    for key in s.keys():\n",
    "        RID_SUMMARY['RID_'+key].append(s[key])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a8c967",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = df.assign(**RID_DETAILS)\n",
    "df = df.assign(**RID_SUMMARY)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81703e24",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s1 = df.shape[0]\n",
    "df = df.drop_duplicates(subset=['Title', 'Artist'])\n",
    "s2 = df.shape[0]\n",
    "print(f'Dropped {s1 - s2} duplicates')\n",
    "\n",
    "raw_songs = df[\"Raw_Lyrics\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78036b4c-f23f-402d-9aa1-b6aed9a616a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = df[df['Nb_Words'] >= 50]\n",
    "s3 = df.shape[0]\n",
    "\n",
    "print(f'Dropped {s2 - s3} songs that were too short')\n",
    "\n",
    "df = df[~df['Title'].str.contains('intro', case=False)]\n",
    "df = df[~df['Title'].str.contains('outro', case=False)]\n",
    "s4 = df.shape[0]\n",
    "\n",
    "print(f'Dropped {s3 - s4} intros and outros')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e3756c-0073-44fd-8cc0-f419330d4423",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def dropping_artists():\n",
    "    # Compute the count of instances for each artist\n",
    "    artist_counts = df['Artist'].value_counts()\n",
    "\n",
    "    # Identify artists with counts less than 50\n",
    "    artists_to_drop = artist_counts[artist_counts < 90].index\n",
    "\n",
    "    # Create a boolean mask indicating rows to be dropped\n",
    "    mask = df['Artist'].isin(artists_to_drop)\n",
    "\n",
    "    # Filter and drop the corresponding rows from the DataFrame\n",
    "    df.drop(df[mask].index, inplace=True)\n",
    "\n",
    "    l1 = len(artists)\n",
    "    # Remove the artists from the artists array\n",
    "    new_artists = [art for art in artists if arts_dict[art] not in artists_to_drop]\n",
    "    dropped_artists = [art for art in artists if arts_dict[art] in artists_to_drop]\n",
    "    for art in dropped_artists:\n",
    "        del arts_dict_op[arts_dict[art]]\n",
    "        del arts_dict[art]\n",
    "\n",
    "    l2 = len(new_artists)\n",
    "\n",
    "\n",
    "    s5 = df.shape[0]\n",
    "    print(f'Dropped {l1 - l2} artists that had too few songs : -{s4 - s5} songs')\n",
    "    \n",
    "dropping_artists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967fc8e0-3186-47de-870c-26b981cc1697",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#FUNCTIONS TO MEASURE SIMILARITY\n",
    "\n",
    "def levenshtein_similarity(text1, text2):\n",
    "    distance = Levenshtein.distance(text1, text2)\n",
    "    max_length = max(len(text1), len(text2))\n",
    "    similarity = (max_length - distance) / (max_length + 1)\n",
    "    return similarity\n",
    "\n",
    "def jaccard_similarity(text1, text2):\n",
    "    set1 = set(text1)\n",
    "    set2 = set(text2)\n",
    "    intersection = len(set1.intersection(set2))\n",
    "    union = len(set1.union(set2))\n",
    "    similarity = intersection / (union + 1)\n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508dc672-b9aa-407a-a7b6-6da8c545d28c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a mask to identify the rows to drop\n",
    "mask = np.zeros(len(df), dtype=bool)\n",
    "\n",
    "\n",
    "for idx1, row1 in df.iterrows():\n",
    "    song1 = row1[\"Raw_Lyrics\"]\n",
    "    title1 = row1[\"Title\"]\n",
    "    artist1 = row1[\"Artist\"]\n",
    "    \n",
    "    # Filter rows with the same title but different artist\n",
    "    duplicate_rows = df[(df[\"Title\"] == title1) & (df[\"Artist\"] != artist1)]\n",
    "    \n",
    "    \n",
    "    # Calculate similarity scores between song1 and duplicate rows\n",
    "    similarity_scores_lev = duplicate_rows[\"Raw_Lyrics\"].apply(lambda x: levenshtein_similarity(song1, x))\n",
    "    \n",
    "    # Identify rows with similarity scores above 0.8\n",
    "    similar_rows_lev = duplicate_rows[similarity_scores_lev > 0.8]\n",
    "\n",
    "    \n",
    "    \n",
    "    df.drop(similar_rows_lev.index, inplace=True)\n",
    "\n",
    "# Drop the marked rows using the mask\n",
    "#df.drop(df.index[mask], inplace=True)\n",
    "\n",
    "s3 = df.shape[0]\n",
    "print(f'Dropped {s2 - s3} covers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ab5476-dced-43ae-bf74-b8cf61e936f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "artist_counts = df['Artist'].value_counts()\n",
    "\n",
    "artist_most_songs = artist_counts.idxmax()\n",
    "most_songs_count = artist_counts.max()\n",
    "\n",
    "artist_least_songs = artist_counts.idxmin()\n",
    "least_songs_count = artist_counts.min()\n",
    "\n",
    "print(f\"Artist with the most songs: {artists[artist_most_songs]} ({most_songs_count} songs)\")\n",
    "print(f\"Artist with the least songs: {artists[artist_least_songs]} ({least_songs_count} songs)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c1e4da-7f3b-404b-96a8-551e1c56c479",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def undersampling():\n",
    "    # Identify artists with a total number of songs greater than 130\n",
    "    artists_above_threshold = artist_counts[artist_counts > 130].index\n",
    "\n",
    "   \n",
    "    for artist in artists_above_threshold:\n",
    "        # Get the indices of songs by the current artist\n",
    "        artist_indices = df[df['Artist'] == artist].index\n",
    "\n",
    "        # Calculate the number of songs to be dropped\n",
    "        songs_to_drop = len(artist_indices) - 130\n",
    "\n",
    "        # Randomly select and drop songs until count reaches 130\n",
    "        songs_to_keep = np.random.choice(artist_indices, size=130, replace=False)\n",
    "        df.drop(artist_indices.difference(songs_to_keep), inplace=True)\n",
    "\n",
    "    s7 = df.shape[0]\n",
    "    print(f'Dropped {s6 - s7} songs from undersampling too represented artists')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c85102e-9aa7-4e30-b9d6-2fccb604a041",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"AFTER CLEANING\\n\")\n",
    "\n",
    "print(f'Number of songs : {df.shape[0]}')\n",
    "print(f'Number of artists : {len(arts_dict)}')\n",
    "print(f'Average number of songs per artist : {int((df.shape[0])/len(arts_dict))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc134cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_box_graph([\"RID_EMOTIONS\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87676540",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_rid = df.filter(regex=r'^RID')\n",
    "df_rid.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea8efe2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Instantiate PCA with the number of components set to the total number of features\n",
    "pca = PCA(n_components=len(df_rid.columns))\n",
    "\n",
    "# Fit PCA on the standardized features\n",
    "pca.fit(df_rid)\n",
    "\n",
    "# Get the explained variance ratio of each principal component\n",
    "explained_variance_ratio = pca.explained_variance_ratio_\n",
    "\n",
    "# Sort the variance ratios in descending order\n",
    "sorted_variance_ratio = sorted(explained_variance_ratio, reverse=True)\n",
    "\n",
    "# Get the indices of the top 10 features with the most variance\n",
    "top_10_indices = np.argsort(explained_variance_ratio)[::-1][:10]\n",
    "\n",
    "# Keep only the top 10 features in the DataFrame\n",
    "df_top_10 = df_rid.iloc[:, top_10_indices]\n",
    "df_top_10.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d84a7c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_box_graph([\"Nb_Words\"], save = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f3aa6a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df[\"Nb_Words_Per_Line\"] = df[\"Nb_Words\"]/df[\"Nb_Lines\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14686b3f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_box_graph([\"Nb_Words_Per_Line\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "63db1788",
   "metadata": {},
   "source": [
    "We add a new feature: Type-Token ratio. It shows how repetitive a song is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324a5b65",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "songs = df[\"Lyrics\"].to_list()\n",
    "songs_flatten = [[word for line in song for word in line] for song in songs]\n",
    "nb_words = df[\"Nb_Words\"].to_list()\n",
    "\n",
    "#unique word means appears only once in the song\n",
    "TTR = []\n",
    "for song, l in zip(songs_flatten, nb_words):\n",
    "    dic = {word: 0 for word in song}\n",
    "    non_unique_words = set()\n",
    "    for word in song:\n",
    "        dic[word] += 1\n",
    "        if dic[word] > 1: #if it appeared more than once, add it\n",
    "            non_unique_words.add(word)\n",
    "    nb_unique_words = l - len(non_unique_words)\n",
    "    ttr = nb_unique_words / l\n",
    "    TTR.append(ttr)\n",
    "    \n",
    "df['TTR'] = TTR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62543d59-5a49-424d-813f-b850dcf3151d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if NUMBER_OF_ARTISTS > 10:\n",
    "    sample = random.sample(list(arts_dict_op.keys()), 10)\n",
    "else:\n",
    "    sample = random.sample(list(arts_dict_op.keys()), NUMBER_OF_ARTISTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4174184b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_box_graph([\"TTR\"], save = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec84283-d73f-491c-8e11-fc7b37b7446e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "raw_songs = df[\"Raw_Lyrics\"].to_list()\n",
    "nb_lines = df[\"Nb_Lines\"].to_list()\n",
    "\n",
    "lines_similarity_jacc = []\n",
    "#lines_similarity_lev= []\n",
    "\n",
    "examples = []\n",
    "\n",
    "for song, num_lines in zip(songs, nb_lines):\n",
    "    line_similarity_count_jacc = 0\n",
    "    \n",
    "    for line1, line2 in itertools.combinations(song, 2):\n",
    "        similarity_jacc = jaccard_similarity(line1, line2)\n",
    "        \n",
    "        \n",
    "        if  similarity_jacc > 0.5:  # Adjust the threshold as needed\n",
    "            line_similarity_count_jacc += 1\n",
    "            examples.append([line1, line2])\n",
    "    \n",
    "    lines_similarity_jacc.append(line_similarity_count_jacc)\n",
    "    \n",
    "\n",
    "df[\"Lines_similarity\"] = lines_similarity_jacc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b291fd-ff60-436c-ad31-10150697da50",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "reduced_examples = random.choices(examples, k=5)\n",
    "for ex in reduced_examples:\n",
    "    for l in ex:\n",
    "        print(l)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007bc8df-01df-4df3-9c39-1d2406576060",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_bar_graph(\"Lines_similarity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c735fe43-2529-49f9-a8b1-bdc86c3a2ae2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "filtered_df = df.loc[df[\"Lines_similarity\"] > 1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a983fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "supertags_dict = {\n",
    "    'VB': 'V', 'VBD': 'V', 'VBG': 'V', 'VBN': 'V', 'VBP': 'V', 'VBZ': 'V', #verbs\n",
    "    'NN': 'N', 'NNS': 'N', 'NNP': 'N', 'NNPS': 'N', #nouns\n",
    "    'RB': 'ADV', 'RBR': 'ADV', 'RBS': 'ADV', 'WRB': 'ADV', #adverbs\n",
    "    'JJ': 'ADJ', 'JJR': 'ADJ', 'JJS': 'ADJ', #adjectives\n",
    "    'DT': 'DET', 'PDT': 'DET', 'WDT': 'DET', #determinents\n",
    "    'PRP': 'PRON', 'PRP$': 'PRON', 'WP': 'PRON', 'WP$': 'PRON'} #pronouns\n",
    "\n",
    "supertags = ['N', 'V', 'ADV', 'ADJ', 'DET', 'PRON']\n",
    "chunk_tags = ['GPE', 'PERSON', 'ORGANIZATION', 'LOCATION', 'FACILITY', 'GSP']\n",
    "\n",
    "#'GPE': 'Geopolitical Entity: Refers to countries, cities, states, or regions.',\n",
    "#'PERSON': 'Person: Refers to individuals or groups of people.',\n",
    "#'ORGANIZATION': 'Organization: Refers to named entities representing companies, institutions, or groups.',\n",
    "#'LOCATION': 'Location: Refers to specific places or locations.',\n",
    "#'FACILITY': 'Facility: Refers to buildings, structures, or other physical facilities.',\n",
    "#'GSP': 'General Single Purpose: Refers to a general chunk tag with a single-purpose label.'\n",
    "\n",
    "def split_sentences(text):\n",
    "\n",
    "    text = re.sub(r'\\([^()]*\\)', ' ', text)\n",
    "    text = text.replace('(', ' ')\n",
    "    text = text.replace(')', ' ')\n",
    "    text = text.replace('  ', ' ')\n",
    "    sentences = re.split(r'(?:[.!?])|\\n\\n', text)\n",
    "    sentences = [s.strip() for s in sentences if len(s)!=0]\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b4814c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def pos_chunk(text):\n",
    "    # Tokenize into sentences and words\n",
    "    sentences = split_sentences(text)\n",
    "    words = [nltk.word_tokenize(sent) for sent in sentences]\n",
    "    \n",
    "    # POS tagging\n",
    "    pos_tags = [nltk.pos_tag(sent) for sent in words]\n",
    "    \n",
    "    # Chunking\n",
    "    chunked_sentences = [nltk.ne_chunk(pos_tag_sent) for pos_tag_sent in pos_tags]\n",
    "\n",
    "    # Calculate POS tag distribution\n",
    "    pos_tag_distribution = {suptag: 0 for suptag in supertags}\n",
    "    for sent in pos_tags:\n",
    "        pos_tag_distribution.update({supertags_dict[tag]: pos_tag_distribution.get(supertags_dict[tag], 0) + 1\n",
    "                                     for _, tag in sent if tag in supertags_dict})\n",
    "\n",
    "    # Calculate chunk tag distribution\n",
    "    chunk_tag_distribution = {chunk_tag: 0 for chunk_tag in chunk_tags}\n",
    "    for sent in chunked_sentences:\n",
    "        chunk_tag_distribution.update({chunk.label(): chunk_tag_distribution.get(chunk.label(), 0) + 1\n",
    "                                       for chunk in sent if hasattr(chunk, 'label') and chunk.label() in chunk_tags})\n",
    "\n",
    "    return pos_tag_distribution, chunk_tag_distribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1597d962",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "raw_songs = df[\"Raw_Lyrics\"].to_list()\n",
    "#df = df.drop(\"Raw_Lyrics\", axis = 1)\n",
    "nb_words = df[\"Nb_Words\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d81c3d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "POS = {'POS_'+key : [] for key in supertags}\n",
    "\n",
    "CHUNK = {'CHUNK_'+key : [] for key in chunk_tags}\n",
    "\n",
    "for raw, l in zip(raw_songs, nb_words):\n",
    "    pos, chunk = pos_chunk(raw)\n",
    "    for key in pos.keys():\n",
    "        POS['POS_'+key].append(pos[key] / l)\n",
    "        \n",
    "    for key in chunk.keys():\n",
    "        CHUNK['CHUNK_'+key].append(chunk[key] / l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0cee47d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = df.assign(**POS)\n",
    "df = df.assign(**CHUNK)\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b8f5e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_box_graph([\"POS_N\", \"POS_V\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be19d104",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_bar_graph(\"CHUNK_PERSON\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5acf332-8b10-4c64-8cb7-62f3bae46950",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_box_graph([\"CHUNK_GPE\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0c2729bd",
   "metadata": {},
   "source": [
    "Now we analyze prononces. We count the frequencies of 1st, 2nd, 3rd person pronons and the ratio of 1st person to 2d person to detect dialogue and interpersonal relations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3172da56",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "path_to_dicts = os.path.join(os.getcwd(), \"Dictionaries\")\n",
    "\n",
    "path = os.path.join(path_to_dicts, \"rare-filtered.txt\")\n",
    "rare = read_file(path)\n",
    "rare = rare.split(\"\\n\")\n",
    "rare = rare[:-1]\n",
    "\n",
    "path = os.path.join(path_to_dicts, \"slang-filtered.txt\")\n",
    "slang = read_file(path)\n",
    "slang = slang.split(\"\\n\")\n",
    "slang = slang[:-1]\n",
    "\n",
    "slang = set(slang)\n",
    "rare = set(rare)\n",
    "\n",
    "rare_use = []\n",
    "slang_use = []\n",
    "\n",
    "songs_flatten = [flatten(song) for song in songs] \n",
    "                 \n",
    "for song in songs_flatten:\n",
    "    \n",
    "    count_r = 0\n",
    "    count_s = 0\n",
    "    l = len(song)\n",
    "    for w in song:\n",
    "        if w in slang:\n",
    "            count_s += 1\n",
    "        if w in rare:\n",
    "            count_r +=1\n",
    "    slang_use.append(count_s/l)\n",
    "    rare_use.append(count_r/l)\n",
    "    \n",
    "\n",
    "df['Slang'] = slang_use\n",
    "df['Rare'] = rare_use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54753de2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_box_graph([\"Rare\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e55fc9e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_bar_graph(\"Slang\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "60c669fd",
   "metadata": {},
   "source": [
    "Now we check whether or not the title appears in the song."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39145513",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ttls = df[\"Title\"].to_list()\n",
    "\n",
    "title_in_song = []\n",
    "\n",
    "raw_songs = [s.lower() for s in raw_songs]\n",
    "\n",
    "\n",
    "for song, title, art in zip(raw_songs, ttls, df[\"Artist\"].to_list()):\n",
    "    pattern = re.escape(title)  # Escape special characters in the title\n",
    "    if re.search(pattern, song):\n",
    "        flag = True\n",
    "    else:\n",
    "        flag = False\n",
    "    title_in_song.append(flag)\n",
    "\n",
    "\n",
    "df['Title_In_Song'] = title_in_song\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e97812",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "t_in = []\n",
    "t_not_in = []\n",
    "total_songs = []\n",
    "\n",
    "for art in sample:\n",
    "    songs_by_artist = df.loc[df['Artist'] == art, 'Title_In_Song'].tolist()\n",
    "    tr = songs_by_artist.count(True)\n",
    "    fa = songs_by_artist.count(False)\n",
    "    total_songs.append(len(songs_by_artist))\n",
    "    t_in.append(tr)\n",
    "    t_not_in.append(fa)\n",
    "\n",
    "xtick_labels = [arts_dict_op[i] for i in sample]\n",
    "\n",
    "X_axis = np.arange(len(xtick_labels))\n",
    "\n",
    "t_in = np.array(t_in)\n",
    "t_not_in = np.array(t_not_in)\n",
    "total_songs = np.array(total_songs)\n",
    "\n",
    "\n",
    "plt.bar(X_axis - 0.2, t_in/total_songs, 0.4, label='Title in lyrics', color = \"gray\")\n",
    "plt.bar(X_axis + 0.2, t_not_in/total_songs, 0.4, label='Title not in lyrics', color=\"yellow\")\n",
    "\n",
    "plt.xticks(X_axis, xtick_labels, rotation='vertical')\n",
    "plt.title('Proportion of songs with the title in the lyrics')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51355259",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sub = []\n",
    "pol = []\n",
    "for song in raw_songs:\n",
    "    scores = TextBlob(song)\n",
    "    pol.append(scores.sentiment.polarity)\n",
    "    sub.append(scores.sentiment.subjectivity)\n",
    "    \n",
    "df[\"Polarity\"] = pol\n",
    "df[\"Subjectivity\"] = sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76315949",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_box_graph([\"Polarity\", \"Subjectivity\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "96b600db-1056-4307-9118-fae27c0c5ea1",
   "metadata": {},
   "source": [
    "# SPLIT INTO TRAIN AND TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c8fdec-074c-4a1d-8f0e-54f24220720a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = df.reset_index(drop=True)\n",
    "\n",
    "artists = df['Artist'].unique()\n",
    "\n",
    "train_df, test_df = train_test_split(df, test_size=0.15, stratify=df['Artist'])\n",
    "\n",
    "print(f'Training size : {train_df.shape[0]}')\n",
    "print(f'Testing size : {test_df.shape[0]}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "61e20f3b-e4ef-47cf-b6bd-52a3e540c1a4",
   "metadata": {},
   "source": [
    "# TDIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04455b31",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "documents = []\n",
    "for i in range(len(arts_dict_op)):\n",
    "    d = train_df.loc[train_df['Artist'] == i, 'Lyrics'].tolist()\n",
    "    d = [flatten(song) for song in d]\n",
    "    d = flatten(d)\n",
    "    documents.append(d)\n",
    "    \n",
    "    \n",
    "documents = [' '.join(doc) for doc in documents]\n",
    "\n",
    "df_docs_train = pd.DataFrame({\"All_Lyrics\" : documents, \n",
    "                        \"Artist\" : np.arange(len(arts_dict_op))})\n",
    "\n",
    "df_docs_train.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1e44244f",
   "metadata": {},
   "source": [
    "At this point we need to seperate train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1d160c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "docs = df_docs_train[\"All_Lyrics\"].to_list()\n",
    "\n",
    "cv = CountVectorizer()\n",
    "stem_count_vector = cv.fit_transform(docs)\n",
    "#stem_count_vector = cv.fit_transform(documents)\n",
    "\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "tfidf_transformer.fit_transform(stem_count_vector)\n",
    "\n",
    "# Get the feature names and IDF weights\n",
    "feature_names = cv.get_feature_names_out()\n",
    "idf_weights = tfidf_transformer.idf_\n",
    "\n",
    "df_words_weights = pd.DataFrame({'word': cv.get_feature_names_out(), 'weight': tfidf_transformer.idf_})\n",
    " \n",
    "df_words_weights.sort_values('weight').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5909069-cc0b-4b4e-b3e4-587bc623b999",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_words_weights.sort_values('weight', ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2791386e-18d5-48a1-b778-3b6a5e0acbe4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_words_weights.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2f43e1-8505-4c26-8c5f-b6a8347d78ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec394d15-075d-4312-ad38-852f71a38b6a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tdif_features(songs):\n",
    "    \n",
    "    song_features_list = []\n",
    "\n",
    "    # Process each song\n",
    "    for i, song in enumerate(songs):\n",
    "\n",
    "        # Convert the song to a string\n",
    "        song_text = ' '.join(song)\n",
    "\n",
    "        # Transform the song text to a count vector\n",
    "        song_counts = cv.transform([song_text])\n",
    "\n",
    "        # Compute the TF-IDF features for the song\n",
    "        song_features = tfidf_transformer.transform(song_counts)\n",
    "\n",
    "        # Convert the sparse matrix to a dataframe\n",
    "        song_features_df = pd.DataFrame(song_features.toarray(), columns=feature_names)\n",
    "\n",
    "        # Append the song features to the list\n",
    "        song_features_list.append(song_features_df)\n",
    "\n",
    "    song_features_list = np.array(song_features_list)\n",
    "    song_features_list = song_features_list.reshape((song_features_list.shape[0], song_features_list.shape[2]))\n",
    "    df_tfidf = pd.DataFrame(song_features_list, columns=feature_names)\n",
    "    df_tfidf['Artist'] = df['Artist'].to_list()\n",
    "    \n",
    "    return df_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17607a52-43fc-40fd-af20-f6ac7b42dc3c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_tfidf = tdif_features(songs_flatten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66361d0-2011-46e2-aceb-1d7484e1c871",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de3b70e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "valid_columns = df_tfidf.iloc[:, :-1].select_dtypes(include=np.number)\n",
    "# Calculate the sum of each row\n",
    "sum_array = np.array(valid_columns.sum(axis=1))\n",
    "\n",
    "# Add the sum_array as a new column in the DataFrame\n",
    "df[\"TF_IDF_Score\"] = sum_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f74210",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_box_graph([\"TF_IDF_Score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75eb045",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "to_keep = df_words_weights.iloc[:1000][\"word\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da5b671",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_tfidf_reduced = pd.concat([df_tfidf[column] for column in to_keep], axis=1)\n",
    "\n",
    "# Display the new DataFrame\n",
    "df_tfidf_reduced.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f925e9c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Perform PCA\n",
    "pca = PCA()\n",
    "pca.fit(df_tfidf_reduced)\n",
    "df_transformed = pca.transform(df_tfidf_reduced)\n",
    "\n",
    "# Calculate explained variance ratio\n",
    "explained_variance_ratio = pca.explained_variance_ratio_\n",
    "\n",
    "plt.axhline(y=0.001, color='r', linestyle='--', label='0.01% Ev ratio')\n",
    "\n",
    "# Create scree plot\n",
    "plt.plot(range(1, len(explained_variance_ratio) + 1), explained_variance_ratio*5, marker='', linestyle='-')\n",
    "\n",
    "# Find the index of intersection\n",
    "intersection_index = -1\n",
    "for i in range(len(explained_variance_ratio)):\n",
    "    if explained_variance_ratio[i] <= 0.001:\n",
    "        intersection_index = i\n",
    "        break\n",
    "\n",
    "# Add a vertical line at the intersection point\n",
    "plt.axvline(x=intersection_index + 1, color='g', linestyle='--')\n",
    "plt.text(intersection_index - 50, 0.01, f'{intersection_index + 1}', ha='center')\n",
    "\n",
    "plt.xlabel('Principal Component')\n",
    "plt.ylabel('Explained Variance Ratio')\n",
    "plt.title('Scree Plot')\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "# Plot the index of intersection on the x-axis\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b311ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate cumulative explained variance\n",
    "cumulative_variance = explained_variance_ratio.cumsum()\n",
    "# Create total variance explained plot with a line\n",
    "plt.plot(range(1, len(cumulative_variance) + 1), cumulative_variance, marker='', linestyle='-')\n",
    "plt.xlabel('Number of Principal Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.title('Total Variance Explained')\n",
    "# Add a line at 80% cumulative explained variance\n",
    "plt.axhline(y=0.9, color='r', linestyle='--', label='90% Variance Explained')\n",
    "plt.legend()\n",
    "# Find the index where cumulative variance exceeds 90%\n",
    "idx = next(x for x, value in enumerate(cumulative_variance) if value >= 0.9)\n",
    "# Add a vertical line at the intersection point\n",
    "plt.axvline(x=idx + 1, color='g', linestyle='--')\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "# Plot the index of intersection on the x-axis\n",
    "plt.text(idx + 1, 0.6, f'{idx + 1}', ha='left', va='center')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa9ef88",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_comp = 150\n",
    "pca = PCA(n_components=n_comp)\n",
    "df_tfidf_pca = pca.fit_transform(df_tfidf_reduced)\n",
    "\n",
    "df_filtered_pca = pd.DataFrame(df_tfidf_pca, columns=[f'PC{i}' for i in range(1, n_comp+1)])\n",
    "\n",
    "df_filtered_pca.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3ffc9d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#############\n",
    "# new data frame containing all the numerical features\n",
    "############\n",
    "new_df = pd.concat([df.reset_index(drop=True), df_filtered_pca.reset_index(drop=True)], axis = 1)\n",
    "new_df = new_df.reset_index(drop=True)\n",
    "new_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3fca02",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "numerical_df = new_df.drop(['Artist', 'Lyrics', 'Title', 'Raw_Lyrics'], axis=1)\n",
    "numerical_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e707dd2-e47e-4222-8f80-b3e97ba546a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if NUMBER_OF_ARTISTS > 10:\n",
    "    sample = random.sample(list(arts_dict_op.keys()), 10)\n",
    "else:\n",
    "    sample = random.sample(list(arts_dict_op.keys()), NUMBER_OF_ARTISTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc55e75-6b40-4935-92d9-08001703b32b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y = new_df[\"Artist\"].to_list()\n",
    "y = np.array([int(yy) for yy in y])\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "numerical_df_scaled = scaler.fit_transform(numerical_df)\n",
    "\n",
    "pca = PCA(n_components=2).fit(numerical_df_scaled)\n",
    "data2D = pca.transform(numerical_df_scaled)\n",
    "\n",
    "artist_labels = [arts_dict_op[i] for i in sample]\n",
    "\n",
    "# Create separate scatter plots for each unique label\n",
    "colors = plt.cm.rainbow(np.linspace(0, 1, len(sample)))  # Generate a range of colors\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "for label, art, color in zip(sample, artist_labels, colors):\n",
    "    mask = (y == label)\n",
    "    scatter = ax.scatter(data2D[:, 0][mask], data2D[:, 1][mask], c=[color], label=art)\n",
    "\n",
    "ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "ax.set_xlabel('First principal component')\n",
    "ax.set_ylabel('Second principal component')\n",
    "\n",
    "# Save the plot as an image\n",
    "#save_path = os.path.join(save_dir, 'PCA_all_feat_2.png')\n",
    "#plt.savefig(save_path, bbox_inches='tight', bbox_extra_artists=[ax.legend_])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37a3c7c-1e61-4d08-8131-e858430d01f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "numerical_df_scaled = scaler.fit_transform(numerical_df)\n",
    "\n",
    "pca = PCA(n_components=2).fit(numerical_df_scaled)\n",
    "data2D = pca.transform(numerical_df_scaled)\n",
    "\n",
    "import itertools\n",
    "import random\n",
    "\n",
    "# Define the sampled indices and artist labels for each graph\n",
    "combinations = list(itertools.combinations(arts_dict_op.keys(), 2))\n",
    "random.shuffle(combinations)\n",
    "sampled_indices = combinations[:4]\n",
    "artist_labels_list = [[arts_dict_op[i] for i in indices] for indices in sampled_indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522ad3d8-d717-445d-aa3d-788e3ad1f389",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Assign red and yellow to the first and second labels, respectively\n",
    "colors_list = ['yellow', 'black']\n",
    "\n",
    "# Plot the graphs using a loop\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "for i, indices in enumerate(sampled_indices):\n",
    "    plt.subplot(2, 2, i + 1)\n",
    "    for j, label in enumerate(indices):\n",
    "        mask = (y == label)\n",
    "        color = colors_list[j % 2]  # Alternate between red and yellow\n",
    "        scatter = plt.scatter(data2D[:, 0][mask], data2D[:, 1][mask], c=color, label=artist_labels_list[i][j])\n",
    "    plt.xlabel('First principal component')\n",
    "    plt.ylabel('Second principal component')\n",
    "    plt.legend()\n",
    "    #save_path = os.path.join(save_dir, f'plot_2arts_PCA2_second.png')\n",
    "    #plt.savefig(save_path)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5cb905-642a-4427-846f-a76d93cc002e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_tfidfvector(df):\n",
    "    if 'TFIDF_Vector' in df.columns:\n",
    "        df.drop('TFIDF_Vector', inplace=True, axis=1)\n",
    "\n",
    "    pca_arrays = []\n",
    "    for index, row in df.iterrows():\n",
    "        pca_columns = row[df.columns.str.startswith('PC')].values\n",
    "        pca_arrays.append(np.array(pca_columns))\n",
    "\n",
    "    df['TFIDF_Vector'] = pca_arrays\n",
    "\n",
    "create_tfidfvector(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efdd3fb-f39a-4b3c-8c2b-139e52651244",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train_indices = train_df.index\n",
    "# test_indices = test_df.index\n",
    "# train_df = new_df.iloc[train_indices]\n",
    "# test_df = new_df.iloc[test_indices]\n",
    "\n",
    "train_indices = train_df.index\n",
    "test_indices = test_df.index\n",
    "train_df = new_df.iloc[train_indices]\n",
    "test_df = new_df.iloc[test_indices]\n",
    "('TFIDF_Vector' in train_df.columns)\n",
    "('Lines_similarity' in train_df.columns)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cff20051-42f6-4a72-a218-49702373eb39",
   "metadata": {},
   "source": [
    "# GET THE TESTING DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a965e8-3347-422e-997f-0714b99f5450",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "# x_train = train_df.drop(['Artist', 'Lyrics', 'Title', 'Raw_Lyrics'], axis=1)\n",
    "# #x_train = pd.DataFrame(scaler.fit_transform(x_train), columns=x_train.columns)\n",
    "# y_train = np.array(train_df['Artist'].to_list())\n",
    "\n",
    "# x_test = test_df.drop(['Artist', 'Lyrics', 'Title', 'Raw_Lyrics'], axis=1)\n",
    "# #x_test = pd.DataFrame(scaler.fit_transform(x_test), columns=x_test.columns)\n",
    "# y_test = np.array(test_df['Artist'].tolist())\n",
    "\n",
    "num_train = train_df.drop(['Lyrics', 'Title', 'Raw_Lyrics'], axis=1)\n",
    "#x_train = pd.DataFrame(scaler.fit_transform(x_train), columns=x_train.columns)\n",
    "\n",
    "num_test = test_df.drop(['Lyrics', 'Title', 'Raw_Lyrics'], axis=1)\n",
    "#x_test = pd.DataFrame(scaler.fit_transform(x_test), columns=x_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a62051-edfe-4e74-886e-eba641b0a802",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Count the occurrences of each artist in the test set\n",
    "artist_counts_test = num_test['Artist'].value_counts()\n",
    "\n",
    "# Count the occurrences of each artist in the train set\n",
    "artist_counts_train = num_train['Artist'].value_counts()\n",
    "\n",
    "# Create a figure with two subplots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "# Plot the artist frequencies for the test set\n",
    "axes[0].bar(artist_counts_test.index, artist_counts_test.values)\n",
    "axes[0].set_xlabel('Artist')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title('Frequency of Artists (Test Set)')\n",
    "\n",
    "# Plot the artist frequencies for the train set\n",
    "axes[1].bar(artist_counts_train.index, artist_counts_train.values)\n",
    "axes[1].set_xlabel('Artist')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].set_title('Frequency of Artists (Train Set)')\n",
    "\n",
    "# Adjust the layout and spacing between subplots\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0849e9ff-712d-422b-97fe-1d15a6b651ed",
   "metadata": {},
   "source": [
    "# KNN STARTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b49e53d-2ed2-4925-83b0-6aea10e35ae9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def learn_knn(**kwargs):\n",
    "    \n",
    "    model = neighbors.KNeighborsClassifier(**kwargs)\n",
    "    model.fit(x_train, y_train)\n",
    "    y_pred = model.predict(x_test)\n",
    "    acc = metrics.accuracy_score(y_test, y_pred)\n",
    "\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6069041c-24d3-4e5b-a9ee-b58df1e63c2e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_knn():\n",
    "    w_accu=[]\n",
    "    nw_accu=[]\n",
    "    l1_nw_accu=[]\n",
    "    l1_w_accu=[]\n",
    "    max_nei = 50\n",
    "    big = (0,0,0)\n",
    "\n",
    "    for i in range(1,max_nei):\n",
    "\n",
    "        w_accu.append(learn_knn(n_neighbors=i, weights='distance'))\n",
    "        nw_accu.append(learn_knn(n_neighbors=i))\n",
    "        l1_w_accu.append(learn_knn(n_neighbors=i, weights='distance', p=1))\n",
    "        l1_nw_accu.append(learn_knn(n_neighbors=i, p=1))\n",
    "\n",
    "        if w_accu[-1] > big[0]:\n",
    "            big = (w_accu[-1], i, 0)\n",
    "\n",
    "        if nw_accu[-1] > big[0]:\n",
    "            big = (nw_accu[-1], i, 1)\n",
    "\n",
    "        if l1_w_accu[-1] > big[0]:\n",
    "            big = (l1_w_accu[-1], i, 2)\n",
    "\n",
    "        if l1_nw_accu[-1] > big[0]:\n",
    "            big = (l1_nw_accu[-1], i, 3)\n",
    "\n",
    "    best_acc = big[0]\n",
    "    best_nei = big[1]\n",
    "    best_param = big[2]\n",
    "\n",
    "    if best_param == 0:\n",
    "        best_param = \"weighted\"\n",
    "    elif best_param == 1:\n",
    "        best_param = \"not weighted\"\n",
    "    elif best_param == 2:\n",
    "        best_param = \"weighted, L1 distance\"\n",
    "    elif best_param == 3:\n",
    "        best_param = \"not weighted, L1 distance\"\n",
    "\n",
    "    print(f'Best accuracy: {best_acc}\\nWith: {best_nei} neighbors, parameters: {best_param}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42998b0-2802-4cb7-b237-22c9c8fbd2af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_knn():\n",
    "    # Create a list of x-values for the parameter variations (e.g., parameter values from 1 to 30)\n",
    "    x_values = list(range(1, max_nei))\n",
    "\n",
    "    # Plot the accuracies\n",
    "    plt.plot(x_values, nw_accu, marker='', label = 'not weighted')\n",
    "    plt.plot(x_values, w_accu, marker = '', label = 'weighted')\n",
    "    plt.plot(x_values, l1_nw_accu, marker='', label = 'not weighted, L1 distance')\n",
    "    plt.plot(x_values, l1_w_accu, marker = '', label = 'weighted, L1 distance')\n",
    "\n",
    "    # Add labels and title to the plot\n",
    "    plt.xlabel('Number of neighbours')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08448f0-fe7d-4d5a-ad72-1f4a0e099434",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test_knn():\n",
    "    model = neighbors.KNeighborsClassifier(n_neighbors=best_nei, weights= 'distance', p=1)\n",
    "    model.fit(x_train, y_train)\n",
    "    scores = cross_val_score(model, x_train, y_train, cv=5)\n",
    "    print(\"Accuracy scores:\", scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25dfe8cc-08c7-46fc-bcb5-f131758ed9f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#y_pred = model.predict(x_test)\n",
    "#accuracy = sklearn.metrics.accuracy_score(y_test, y_pred)\n",
    "#print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca41866c-c5d0-4b9c-9fce-aaf71364e9b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def confusion_matrix(y_test, y_pred):\n",
    "    \n",
    "     \n",
    "    y_test_labels = [arts_dict_op[y] for y in y_test]\n",
    "    y_pred_labels = [arts_dict_op[y] for y in y_pred]\n",
    "    \n",
    "    \n",
    "    confusion_matrix = metrics.confusion_matrix(y_true=y_test_labels, y_pred=y_pred_labels)\n",
    "    \n",
    "    #index_labels = list(map(lambda artist: 'actual ' + artist, artists))\n",
    "    index_labels = list(map(lambda artist: artist, arts_dict_op.keys()))\n",
    "\n",
    "    #column_labels = list(map(lambda artist: 'predicted ' + artist, artists))\n",
    "    column_labels = list(map(lambda artist: artist, arts_dict_op.keys()))\n",
    "\n",
    "    df = pd.DataFrame(data=confusion_matrix, index=index_labels, columns=column_labels)\n",
    "    \n",
    "    \n",
    "    df.style.set_properties(**{'width':'6em', 'text-align':'center'}).set_table_styles([dict(selector=\"th\", props=[('text-align', 'center')])])\n",
    "   \n",
    "    return df\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "86aef3f6-2f52-4982-9a83-f3ba2ec10411",
   "metadata": {},
   "source": [
    "# LOGISTIC REGRESSION STARTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8abfa8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def n_sets_func(nsets): \n",
    "    artists_set = random.sample(list(arts_dict_op.keys()), nsets)\n",
    "    pos_w = (nsets-1)/nsets\n",
    "    neg_w = 1/nsets\n",
    "    return artists_set, pos_w, neg_w\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b89af4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def n_sets_func_mine(df, art_set):    \n",
    "    df2 = df.drop(df.columns[[0, 3, 4]], axis=1)\n",
    "    df2.reset_index(drop=True, inplace=True)\n",
    "    df2 = df2[df2['Artist'].isin(art_set)]\n",
    "    n_art = len(art_set)\n",
    "    pos_w = (n_art-1)/n_art\n",
    "    neg_w = 1/n_art\n",
    "    fts = df2.columns\n",
    "    return df2, fts, pos_w, neg_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d74cfe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def mean_of_arrays(matrix):\n",
    "    mean_values = np.mean(matrix, axis=0)\n",
    "    return mean_values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c114cfc0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def artist_df(artist, df):\n",
    "    \"\"\"\n",
    "    takes the aritst number and the training dataframe\n",
    "    returns the mean and std devaition for all somgs of that artist in that dataframe\n",
    "    \"\"\"\n",
    "    #print(df[\"TFIDF_Vector\"])\n",
    "    art_data = df[df['Artist'] == artist]\n",
    "    arrays = art_data['TFIDF_Vector'].values\n",
    "    #print(arrays)\n",
    "    # Convert the arrays into a matrix\n",
    "    matrix = np.stack(arrays)\n",
    "    mean = np.array(mean_of_arrays(matrix))\n",
    "    # Calculate the average features for artist number 4\n",
    "    average_features_artist = art_data.mean(numeric_only=True)\n",
    "    # Calculate the standard deviation of features for artist number 4\n",
    "    std_features_artist = art_data.std(numeric_only=True)\n",
    "    k= ((pd.DataFrame({'Average': average_features_artist, 'Standard Deviation': std_features_artist})).transpose()).drop(\"Artist\", axis=1)\n",
    "    return k,mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fdf0470",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def cosine_angle(vector1, vector2):\n",
    "    \"\"\"\n",
    "    Similarity between 2 vectors between 0 and 1. To be used for PCAd TFIDF components.\n",
    "    \"\"\"\n",
    "    dot_prod = np.dot(vector1, vector2)\n",
    "    mag1 = np.linalg.norm(vector1)\n",
    "    mag2 = np.linalg.norm(vector2)\n",
    "    if(mag1 *mag2 !=0):\n",
    "        return dot_prod / (mag1 * mag2)\n",
    "    return 0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9eda20",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def std_rows(df):\n",
    "    \"\"\"\n",
    "    Takes a dataframe with the first 2 rows belonging to an artist and the following rows representing songs.\n",
    "    Returns the dataframe to be trained for that artist.\n",
    "    \"\"\"\n",
    "    art_pca = df.iloc[0, -1]\n",
    "\n",
    "    for i in range(2, len(df)):\n",
    "        df.iloc[i, -1] = cosine_angle(art_pca, df.iloc[i, -1])\n",
    "\n",
    "    first_row = df.iloc[0, :-1]\n",
    "    s_row = df.iloc[1, :-1]\n",
    "\n",
    "    mask = s_row != 0  # Create a mask to avoid division by zero\n",
    "    df.iloc[2:, :-1] = (first_row - df.iloc[2:, :-1]) / np.where(mask, s_row, 1)\n",
    "\n",
    "    df = df.iloc[2:].reset_index(drop=True)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad8061c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#### WIll make the dataframe of songs for any artist\n",
    "def df_maker(artist, df, df2):\n",
    "    \"\"\" \n",
    "    INPUTS\n",
    "    artist- aritst number, int\n",
    "    df- dataframe with songs and their features\n",
    "    output- dataframe with featurs of the song wrt the aritst\n",
    "    \"\"\"\n",
    "    art_df, meanpca = artist_df(artist, df2)\n",
    "    art_df[\"TFIDF_Vector\"] = [meanpca, None]\n",
    "    comb_df = pd.concat([art_df, df])\n",
    "    return (std_rows(comb_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44109b0e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def top_K_accs(y_test, arr):\n",
    "    \n",
    "    y_test = np.array(y_test)\n",
    "    l = len(y_test)\n",
    "    top_accs=[]\n",
    "    for max_indices in arr:\n",
    "        s=0\n",
    "        for i in range(0,l):\n",
    "            if y_test[i] in max_indices[i]:\n",
    "                s+=1\n",
    "        top_accs.append(100*s/l)\n",
    "    return(top_accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23c82cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def logistic_test(artists_set, top, x_train, x_test):\n",
    "\n",
    "    df2, _, pos_w, neg_w = n_sets_func_mine(df=x_train, art_set = artists_set)\n",
    "    x_train = df2.drop('Artist', axis=1)\n",
    "    y_train = df2['Artist']\n",
    "\n",
    "    \n",
    "    df3, _, pos_w, neg_w = n_sets_func_mine(df=x_test, art_set = artists_set)\n",
    "    x_test = df3.drop('Artist', axis=1)\n",
    "    y_test = df3['Artist']\n",
    "    fts = x_train.columns\n",
    "    \n",
    "    probs =[]\n",
    "    coeffs=[]\n",
    "\n",
    "    for i in artists_set:\n",
    "        X2 = df_maker(artist = i,df = x_train, df2= df2)\n",
    "        X2 = pd.concat([X2.reset_index(drop=True), x_train.iloc[:,:-1].reset_index(drop=True)], axis=1)\n",
    "        X3 = (df_maker(artist =i,df = x_test, df2=df2))\n",
    "        X3 = pd.concat([X3.reset_index(drop=True), x_test.iloc[:, :-1].reset_index(drop=True)], axis =1)\n",
    "        y2 = np.array([1 if x == i else 0 for x in y_train])\n",
    "        # Split the data into training and testing sets\n",
    "        logreg = LogisticRegression(max_iter=5000, solver='lbfgs', class_weight={0: neg_w, 1: pos_w})\n",
    "        logreg.fit(X2, y2)\n",
    "        coefficients = logreg.coef_\n",
    "        coeffs.append(coefficients[0])\n",
    "        probs.append(logreg.predict_proba(X3)[:, 1])\n",
    "    coeffs = np.vstack(coeffs)\n",
    "    coeffs= np.mean(coeffs, axis=0)\n",
    "    probs = np.array(probs)\n",
    "    probs_by_tops=[]\n",
    "    for i in top:\n",
    "        Q = (np.argsort(probs, axis=0)[-i:, :]).transpose()\n",
    "        temp = []\n",
    "        for i in Q:\n",
    "            temp2=[]\n",
    "            for j in i:\n",
    "                temp2.append(artists_set[j])\n",
    "            temp.append(temp2)\n",
    "        probs_by_tops.append(temp)\n",
    "    return probs_by_tops, top_K_accs(y_test=y_test,arr= probs_by_tops), coeffs, y_test, fts, artists_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362cafb1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def logistic_train_acc(artists_set, top, x_train):\n",
    "\n",
    "    df2, _, pos_w, neg_w = n_sets_func_mine(df=x_train, art_set = artists_set)\n",
    "    x_train = df2.drop('Artist', axis=1)\n",
    "    y_train = df2['Artist']\n",
    "\n",
    "    probs =[]\n",
    "\n",
    "    for i in artists_set:\n",
    "        X2 = df_maker(artist = i,df = x_train, df2= df2)\n",
    "        X2 = pd.concat([X2.reset_index(drop=True), x_train.iloc[:,:-1].reset_index(drop=True)], axis=1)\n",
    "        y2 = np.array([1 if x == i else 0 for x in y_train])\n",
    "        # Split the data into training and testing sets\n",
    "        logreg = LogisticRegression(max_iter=5000, solver='lbfgs', class_weight={0: neg_w, 1: pos_w})\n",
    "        logreg.fit(X2, y2)\n",
    "        probs.append(logreg.predict_proba(X2)[:, 1])\n",
    "    probs = np.array(probs)\n",
    "    probs_by_tops=[]\n",
    "    for i in top:\n",
    "        Q = (np.argsort(probs, axis=0)[-i:, :]).transpose()\n",
    "        temp = []\n",
    "        for i in Q:\n",
    "            temp2=[]\n",
    "            for j in i:\n",
    "                temp2.append(artists_set[j])\n",
    "            temp.append(temp2)\n",
    "        probs_by_tops.append(temp)\n",
    "    return top_K_accs(y_test=y_train,arr= probs_by_tops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877ae548",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def logistic_test_random_bagging(artists_set, top, x_train, x_test, nest, max_sam, max_fts, oob, warm):\n",
    "\n",
    "    df2, _, pos_w, neg_w = n_sets_func_mine(df=x_train, art_set = artists_set)\n",
    "    x_train = df2.drop('Artist', axis=1)\n",
    "    y_train = df2['Artist']\n",
    "\n",
    "    df3, _, pos_w, neg_w = n_sets_func_mine(df=x_test, art_set = artists_set)\n",
    "    x_test = df3.drop('Artist', axis=1)\n",
    "    y_test = df3['Artist']\n",
    "    fts = x_train.columns\n",
    "    \n",
    "    probs =[]\n",
    "\n",
    "    for i in artists_set:\n",
    "        X2 = df_maker(artist = i,df = x_train, df2= df2)\n",
    "        X2 = pd.concat([X2.reset_index(drop=True), x_train.iloc[:,:-1].reset_index(drop=True)], axis=1)\n",
    "        X3 = (df_maker(artist =i,df = x_test, df2=df2))\n",
    "        X3 = pd.concat([X3.reset_index(drop=True), x_test.iloc[:, :-1].reset_index(drop=True)], axis =1)\n",
    "        y2 = np.array([1 if x == i else 0 for x in y_train])\n",
    "        # Split the data into training and testing sets\n",
    "        logreg = LogisticRegression(max_iter=5000, solver='lbfgs', class_weight={0: neg_w, 1: pos_w})\n",
    "        logreg_bagging = BaggingClassifier(estimator=logreg, n_estimators=nest, max_samples=max_sam, max_features=max_fts, oob_score = oob, warm_start=warm)\n",
    "        logreg_bagging.fit(X2, y2)\n",
    "        probs.append(logreg_bagging.predict_proba(X3)[:, 1])\n",
    "    probs = np.array(probs)\n",
    "    probs_by_tops=[]\n",
    "    for i in top:\n",
    "        Q = (np.argsort(probs, axis=0)[-i:, :]).transpose()\n",
    "        temp = []\n",
    "        for i in Q:\n",
    "            temp2=[]\n",
    "            for j in i:\n",
    "                temp2.append(artists_set[j])\n",
    "            temp.append(temp2)\n",
    "        probs_by_tops.append(temp)\n",
    "    return  top_K_accs(y_test=y_test,arr= probs_by_tops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001b8c6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2cd559",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def logistic_test_random_boosting(artists_set, top, x_train, x_test):\n",
    "    \n",
    "\n",
    "    df2, _, pos_w, neg_w = n_sets_func_mine(df=x_train, art_set = artists_set)\n",
    "    x_train = df2.drop('Artist', axis=1)\n",
    "    y_train = df2['Artist']\n",
    "\n",
    "    df3, _, pos_w, neg_w = n_sets_func_mine(df=x_test, art_set = artists_set)\n",
    "    x_test = df3.drop('Artist', axis=1)\n",
    "    y_test = df3['Artist']\n",
    "    \n",
    "    probs =[]\n",
    "\n",
    "    for i in artists_set:\n",
    "        X2 = df_maker(artist = i,df = x_train, df2= df2)\n",
    "        X2 = pd.concat([X2.reset_index(drop=True), x_train.iloc[:,:-1].reset_index(drop=True)], axis=1)\n",
    "        X3 = (df_maker(artist =i,df = x_test, df2=df2))\n",
    "        X3 = pd.concat([X3.reset_index(drop=True), x_test.iloc[:, :-1].reset_index(drop=True)], axis =1)\n",
    "        y2 = np.array([1 if x == i else 0 for x in y_train])\n",
    "        # Split the data into training and testing sets\n",
    "        adaboost = AdaBoostClassifier(estimator=LogisticRegression(max_iter=5000),n_estimators=50, learning_rate=1)\n",
    "        adaboost.fit(X2, y2)\n",
    "        probs.append(adaboost.predict_proba(X3)[:, 1])\n",
    "    probs = np.array(probs)\n",
    "    probs_by_tops=[]\n",
    "    for i in top:\n",
    "        Q = (np.argsort(probs, axis=0)[-i:, :]).transpose()\n",
    "        temp = []\n",
    "        for i in Q:\n",
    "            temp2=[]\n",
    "            for j in i:\n",
    "                temp2.append(artists_set[j])\n",
    "            temp.append(temp2)\n",
    "        probs_by_tops.append(temp)\n",
    "    return  top_K_accs(y_test=y_test,arr= probs_by_tops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884d804e-8e53-461a-832b-dd69e60c8897",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def misclassy(n, k, l):\n",
    "    \n",
    "    # n : number of artists considered\n",
    "    # k : top k preductions considered\n",
    "    # l : top l misclassified\n",
    "    \n",
    "    misclassified_numbers = Counter()\n",
    "    artists_set = random.sample(list(arts_dict_op.keys()), n)\n",
    "    probs_by_tops, accs, _, y_test, _, artists_set = logistic_test(artists_set, top = [k], x_train = num_train,x_test= num_test)\n",
    "\n",
    "    probs_by_tops = probs_by_tops[0]\n",
    "    for i, true_value in enumerate(y_test):\n",
    "        if true_value not in probs_by_tops[i]:\n",
    "            misclassified_numbers[true_value] += 1\n",
    "\n",
    "    total_samples = Counter(y_test)  # Count occurrences of each number in y_test\n",
    "\n",
    "    misclassification_percentages = {\n",
    "        number: count / total_samples[number] * 100\n",
    "        for number, count in misclassified_numbers.items()\n",
    "    }\n",
    "\n",
    "    least_misclassified = sorted(\n",
    "        misclassification_percentages.items(), key=lambda x: x[1]\n",
    "    )[:l]\n",
    "    \n",
    "    top_misclassified = sorted(\n",
    "        misclassification_percentages.items(), key=lambda x: x[1], reverse=True\n",
    "    )[:l]\n",
    "\n",
    "    print(f\"The top {l} most misclassified numbers by percentage are:\\n\")\n",
    "    for number, percentage in top_misclassified:\n",
    "        print(f\"Number: {arts_dict_op[number]}, Misclassification percentage: {percentage:.2f}%\")\n",
    "        \n",
    "    print(f\"The top {l} least misclassified numbers by percentage are:\\n\")\n",
    "    for number, percentage in least_misclassified:\n",
    "        print(f\"Number: {arts_dict_op[number]}, Misclassification percentage: {percentage:.2f}%\")\n",
    "        \n",
    "    return [top[0] for top in top_misclassified], [top[0] for top in least_misclassified]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fed0b8f-4167-4db2-b5ed-712d0f4adf23",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "top, l_top = misclassy(n = 60, k = 8, l = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2b74d8-a241-48e8-bc7d-86233ba34c6e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "top1, l_top1 = misclassy(n = 60, k = 8, l = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925f76f8-8781-4d27-9f08-7e4e145766ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "top2, l_top2 = misclassy(n = 60, k = 8, l = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2dfc2cc-ee33-41a6-9a38-c7dab8680d66",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_mis = set(top + top1 + top2)\n",
    "all_least_mis = set(l_top + l_top1 + l_top2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e99e1da-143b-43c8-b89c-d4a82dc94b55",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_mis = new_df[new_df['Artist'].isin(all_mis|all_least_mis)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb36fa51-6e43-4727-b1b8-afa0a4cf5b60",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_mis = df_mis.drop([\"Lyrics\", \"Raw_Lyrics\", \"TFIDF_Vector\", \"Title\"], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75fb0e73-9e68-4366-b0fd-c2acd20a4a96",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_mis['Mis'] = df_mis['Artist'].isin(all_mis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f329af5-f4d1-404b-960a-a4c0e3b6334d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_mis.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5759ff-6e79-47a5-8877-8036a258dad3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_mis_wart = df_mis.drop('Artist', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa819656-324b-4596-af71-0550872ec12c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "df_mis_wart = pd.DataFrame(scaler.fit_transform(df_mis_wart), columns=df_mis_wart.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df19897c-0712-438d-8873-c0e2ebd39c4d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Group the DataFrame by the 'Mis' column and calculate the mean for each feature\n",
    "feature_means = df_mis_wart.groupby('Mis').mean()\n",
    "\n",
    "# Calculate the absolute difference between the mean values of each feature for the two misclassification classes\n",
    "feature_diff = feature_means.diff().abs()\n",
    "\n",
    "# Sort the features based on the absolute difference in descending order\n",
    "sorted_features = feature_diff.iloc[-1].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44dd7ad3-1b97-4fae-be86-4ed7b76ef710",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "top_10_features = sorted_features.head(10)\n",
    "\n",
    "# Plot the top 10 features in a bar plot\n",
    "plt.figure(figsize=(10, 6))  # Optional: Adjust the figure size\n",
    "top_10_features.plot(kind='bar', color='gray')\n",
    "plt.ylabel('Absolute Difference')\n",
    "plt.title('Top 10 features that change the most between well and baldy classified classes')\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1785d190-1e22-4069-b049-b235fba80d9b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "artists_of_interest = new_df[new_df['Artist'].isin(all_mis)]\n",
    "\n",
    "# Count the number of songs per artist for the artists of interest\n",
    "counts_mis = artists_of_interest['Artist'].value_counts()\n",
    "\n",
    "# Count the number of songs per artist for the other artists\n",
    "other_artists = new_df[~new_df['Artist'].isin(all_mis)]\n",
    "counts_other_artists = other_artists['Artist'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a89ea0-a715-491a-b998-6155ce0f7bde",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "to_plot = [counts_mis.to_list(),counts_other_artists.to_list()]\n",
    "plt.boxplot(to_plot)\n",
    "xtick_labels = ['Badly classfied artists', 'Well classified artists']\n",
    "plt.gca().set_xticklabels(xtick_labels)\n",
    "save_path = os.path.join(save_dir, f'plot_size_diff_mis.png')\n",
    "plt.ylabel(\"Number of songs per artist\")\n",
    "#plt.savefig(save_path)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233df4a3-36cc-4de5-8be1-c3598b9faf53",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "numerical_df_scaled = pd.DataFrame(numerical_df_scaled, columns = numerical_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3355c5-5804-423a-810d-6da596469c3c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "numerical_df_scaled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086b4ef9-b8d7-4a86-9fcc-3b991918021d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "numerical_df_scaled = numerical_df_scaled.reset_index(drop=True)\n",
    "new_df = new_df.reset_index(drop=True)\n",
    "numerical_df_scaled['Artist'] = new_df['Artist'].copy()\n",
    "columns_to_drop = [col for col in df_mis_wart.columns if col.startswith('PC')]\n",
    "numerical_df_scaled = numerical_df_scaled.drop(columns=columns_to_drop)\n",
    "numerical_df_scaled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d247c508-d12a-4597-b0b7-5ee608dd8bf4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "av_songs = int(new_df['Artist'].value_counts().mean())\n",
    "print(f'The median number of songs per artist is {av_songs}')\n",
    "\n",
    "# Get artists with more songs than the median\n",
    "artists_more_than_median = new_df['Artist'].value_counts()[new_df['Artist'].value_counts() > av_songs].index.tolist()\n",
    "\n",
    "# Get artists with fewer songs than the median\n",
    "artists_less_than_median = new_df['Artist'].value_counts()[new_df['Artist'].value_counts() < av_songs].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1fdd24-b5e5-4b7b-8086-b68c3e0121a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "variance_dict = {}  # Dictionary to store feature families and their variances\n",
    "\n",
    "for feature in numerical_df_scaled.columns:\n",
    "    if feature[:2] in {\"Ar\", \"Mi\"}:\n",
    "        continue\n",
    "    if feature[:2] == \"PO\":\n",
    "        family = \"POS feat\"\n",
    "    elif feature[:2] == \"CH\":\n",
    "        family = \"CHUNK feat\"\n",
    "    elif feature[:2] == \"RI\":\n",
    "        family = \"RID feat\"\n",
    "    else:\n",
    "        family = feature\n",
    "        \n",
    "    feat_var_less = []\n",
    "    feat_var_more = []\n",
    "    \n",
    "    for art in artists_more_than_median:\n",
    "        feat = numerical_df_scaled.loc[numerical_df_scaled['Artist'] == art, feature]\n",
    "        var = feat.var()\n",
    "        feat_var_less.append(var)\n",
    "    \n",
    "    for art in artists_less_than_median :\n",
    "        feat = numerical_df_scaled.loc[numerical_df_scaled['Artist'] == art, feature]\n",
    "        var = feat.var()\n",
    "        feat_var_more.append(var)\n",
    "    \n",
    "    if family in variance_dict:\n",
    "        variance_dict[family]['less_med'].extend(feat_var_less)\n",
    "        variance_dict[family]['more_med'].extend(feat_var_more)\n",
    "    else:\n",
    "        variance_dict[family] = {'less_med': feat_var_less, 'more_med': feat_var_more}\n",
    "\n",
    "# Calculate the mean variance for each feature family\n",
    "mean_variances_mis = []\n",
    "mean_variances_least_mis = []\n",
    "family_labels = []\n",
    "\n",
    "for family, variances in variance_dict.items():\n",
    "    mean_var_mis = np.mean(variances['less_med'])\n",
    "    mean_var_least_mis = np.mean(variances['more_med'])\n",
    "    mean_variances_mis.append(mean_var_mis)\n",
    "    mean_variances_least_mis.append(mean_var_least_mis)\n",
    "    family_labels.append(family)\n",
    "\n",
    "# Define the X_axis variable\n",
    "X_axis = np.arange(len(family_labels))\n",
    "\n",
    "# Plot the mean variances for the feature families\n",
    "plt.bar(X_axis, mean_variances_mis, 0.4, label='Less songs than avreage', color = \"grey\")\n",
    "plt.bar(X_axis + 0.4, mean_variances_least_mis, 0.4, label='More songs than avreage', color = \"yellow\")\n",
    "\n",
    "plt.xticks(X_axis, family_labels, rotation=45, ha='right')\n",
    "plt.title('Mean Variance for Feature Families per artist')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26eda39",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "aset =  random.sample(list(arts_dict_op.keys()), 20)\n",
    "bset = random.sample(list(arts_dict_op.keys()), 40)\n",
    "cset = random.sample(list(arts_dict_op.keys()), 10)\n",
    "s=0\n",
    "top=[1]\n",
    "arr=[]\n",
    "nest = 10\n",
    "max_sam = 1\n",
    "max_fts = 1\n",
    "for warm in [True, False]:\n",
    "    if(warm == False):\n",
    "        for oob in [True, False]:\n",
    "            a = logistic_test_random_bagging(artists_set=aset, top = top, nest=nest, x_train = num_train, x_test = num_test, max_sam = max_sam/10, max_fts = max_fts/10, oob = oob, warm = warm)\n",
    "            b = logistic_test_random_bagging(artists_set=bset, top = top, nest=nest, x_train = num_train, x_test = num_test, max_sam = max_sam/10, max_fts = max_fts/10, oob = oob, warm = warm)\n",
    "            c = logistic_test_random_bagging(artists_set=cset, top = top, nest=nest, x_train = num_train, x_test = num_test, max_sam = max_sam/10, max_fts = max_fts/10, oob = oob, warm = warm)\n",
    "            brr=[nest,max_sam,max_fts,False, oob, a+b+c]\n",
    "            print(brr)\n",
    "    else:\n",
    "        a = logistic_test_random_bagging(artists_set=aset, top = top, nest=nest, x_train = num_train, x_test = num_test, max_sam = max_sam/10, max_fts = max_fts/10, oob = False, warm = warm)\n",
    "        b = logistic_test_random_bagging(artists_set=bset, top = top, nest=nest, x_train = num_train, x_test = num_test, max_sam = max_sam/10, max_fts = max_fts/10, oob = False, warm = warm)\n",
    "        c = logistic_test_random_bagging(artists_set=cset, top = top, nest=nest, x_train = num_train, x_test = num_test, max_sam = max_sam/10, max_fts = max_fts/10, oob = False, warm = warm)\n",
    "        brr=[nest,max_sam,max_fts,warm, False, a+b+c]\n",
    "        print(brr)\n",
    "    arr.append(brr)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "223459e6",
   "metadata": {},
   "source": [
    "warm = true has given better results than false in every comparison conducted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c7e5f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot each array of y values\n",
    "def plot_top_tog(brr, nlist, top):\n",
    "    brr2 = brr\n",
    "\n",
    "    max_len = max(len(sublist) for sublist in brr2)\n",
    "    drr = [[] for _ in range(max_len)]\n",
    "\n",
    "    for sublist in brr2:\n",
    "        for i, element in enumerate(sublist):\n",
    "            drr[i].append(element)\n",
    "\n",
    "    brr2=drr\n",
    "\n",
    "    counter=0\n",
    "    for y_values in brr2:\n",
    "        plt.plot(nlist[-len(y_values):], y_values, marker='o', label = \"top \"+str(top[counter])+\" choices\")\n",
    "        counter+=1\n",
    "\n",
    "    # Add labels and title\n",
    "    plt.xlabel('Number of aritsts')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Accuracy of n artists with top k choices')\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "    # Display the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f66fbce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(data, st, name):\n",
    "    features = data['feature']\n",
    "    coefficients = data[name]\n",
    "    \n",
    "    new_features = []\n",
    "    new_coefficients = []\n",
    "    pos_coefficients = []\n",
    "    pos_total = 0.0\n",
    "    \n",
    "    for feature, coefficient in zip(features, coefficients):\n",
    "        if feature.startswith(st):\n",
    "            pos_coefficients.append(coefficient)\n",
    "        else:\n",
    "            new_features.append(feature)\n",
    "            new_coefficients.append(coefficient)\n",
    "    \n",
    "    if pos_coefficients:\n",
    "        pos_total = sum(pos_coefficients)\n",
    "        new_features.append(st)\n",
    "        new_coefficients.append(pos_total)\n",
    "    \n",
    "    processed_data = {\n",
    "        'feature': new_features,\n",
    "        name: new_coefficients\n",
    "    }\n",
    "    \n",
    "    return processed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "badd25cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotter(data, name):\n",
    "    feature_importance_df = pd.DataFrame(data)\n",
    "    sorted_df = feature_importance_df.sort_values(name)\n",
    "    \n",
    "    plt.bar(sorted_df['feature'], sorted_df[name], facecolor='gray', align='center')\n",
    "    plt.xlabel('Feature')  # Reduce the x-axis label size\n",
    "    plt.ylabel(name)  # Reduce the y-axis label size\n",
    "    plt.title(f'{name} vs. Feature')  # Reduce the title size\n",
    "    plt.xticks(rotation=60, ha='right')  # Rotate x-axis labels by 60 degrees and align right\n",
    "    plt.yticks(fontsize=8)  # Reduce the y-axis tick label size\n",
    "    plt.ylim(ymin=min(sorted_df[name]), ymax=max(sorted_df[name]))\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b8da90",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def feature_imp1(crr, fts):    \n",
    "    for coeffs in crr:\n",
    "        fts2 = [(element+\"DN\") for element in fts if element != 'Artist']\n",
    "        for i in range(0,len(fts2)):\n",
    "            if(fts2[i] == \"PCA diff normal\"):\n",
    "                fts2[i] = \"TF_IDF vector similarity\"\n",
    "        fts3 = [element for element in fts if element != 'Artist' and element !='PCA']\n",
    "        fts2+=fts3\n",
    "        fts4=[]\n",
    "        coeffs2=[]\n",
    "        for i in range(0,len(coeffs)):\n",
    "            if(np.abs(coeffs[i])>=0.05):\n",
    "                coeffs2.append(coeffs[i])\n",
    "                fts4.append(fts2[i])\n",
    "        data = {'feature': fts4, 'coefficient': coeffs2}\n",
    "        plotter(data, 'coefficient')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c7579a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_imp2(crr, fts):    \n",
    "    for coeffs in crr:\n",
    "        fts2 = [(element+\" diff normal\") for element in fts if element != 'Artist']\n",
    "        for i in range(0,len(fts2)):\n",
    "            if(fts2[i] == \"PCA diff normal\"):\n",
    "                fts2[i] = \"TF_IDF vector similarity\"\n",
    "        fts3 = [element for element in fts if element != 'Artist' and element !='PCA']\n",
    "        fts2+=fts3\n",
    "        coeffs2 = np.absolute(coeffs)\n",
    "        coeffs2 = coeffs2/np.linalg.norm(coeffs2)\n",
    "        data = {'feature': fts2, 'coefficient': coeffs2}\n",
    "        for bruh in fts3:\n",
    "            data = process_data(data, bruh[:4], 'coefficient')\n",
    "        plotter(data, 'coefficient')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0560d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_imp3(crr, fts):    \n",
    "    for coeffs in crr:\n",
    "        fts2 = [(element+\" diff normal\") for element in fts if element != 'Artist']\n",
    "        for i in range(0,len(fts2)):\n",
    "            if(fts2[i] == \"PCA diff normal\"):\n",
    "                fts2[i] = \"TF_IDF vector similarity\"\n",
    "        fts3 = [element for element in fts if element != 'Artist' and element !='PCA']\n",
    "        fts2+=fts3\n",
    "        coeffs2 = np.absolute(coeffs)\n",
    "        coeffs2 = coeffs2/np.linalg.norm(coeffs2)\n",
    "        coeffs3 = 100*coeffs2/np.sum(coeffs2)\n",
    "        data = {'feature': fts2, 'Percentage': coeffs3}\n",
    "        for bruh in fts3:\n",
    "            data = process_data(data, bruh[:4], 'Percentage')\n",
    "        plotter(data, 'Percentage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a3843f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlist=[2, 4, 8,16]\n",
    "arr=[]\n",
    "brr=[]\n",
    "crr=[]\n",
    "yrr=[]\n",
    "top=[]\n",
    "fts = np.array(df.columns)\n",
    "for n in nlist:\n",
    "    print(n)\n",
    "    top =[2**i for i in range(0, (int)(np.log2(n//2)+1))]\n",
    "    ar,br,cr,y_test,fts = logistic_test(n_art=n,df =df, top = top)\n",
    "    arr.append(ar)\n",
    "    brr.append(br)\n",
    "    crr.append(cr)\n",
    "    yrr.append(y_test)\n",
    "    \n",
    "plot_top_tog(brr = brr, nlist =nlist, top = top)\n",
    "#feature_imp1(crr=crr, fts = fts)\n",
    "#feature_imp2(crr=crr, fts = fts)\n",
    "#feature_imp3(crr=crr, fts = fts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
